{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minimal_bert_dot_usage_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.1 64-bit ('deep_learning': conda)",
      "metadata": {
        "interpreter": {
          "hash": "c850e6f77ff7c9cbece5364f8526ec42dd183cf59251b1cfd7b71a0467b242c1"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng63tDwZSSm5"
      },
      "source": [
        "# Using Our Margin-MSE trained ColBERT Checkpoint\n",
        "\n",
        "We provide a fully retrieval trained (with Margin-MSE using a 3 teacher Bert_Cat Ensemble on MSMARCO-Passage) DistilBert-based instance on the HuggingFace model hub here: https://huggingface.co/sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco\n",
        "\n",
        "This instance can be used to **re-rank a candidate set** or **directly for a vector index based dense retrieval**. The architecure is a 6-layer DistilBERT, with an additional single linear layer at the end. \n",
        "\n",
        "If you want to know more about our simple, yet effective knowledge distillation method for efficient information retrieval models for a variety of student architectures, check out our paper: https://arxiv.org/abs/2010.02666 üéâ\n",
        "\n",
        "This notebook gives you a minimal usage example of downloading our ColBERT checkpoint to encode passages and queries to create a (term-x-term dot-product & max-pool & sum) score of their relevance. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's get started by installing the awesome *transformers* library from HuggingFace:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2WyNOE2R2rW"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqkWDa_jWu7c"
      },
      "source": [
        "The next step is to download our checkpoint and initialize the tokenizer and models:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTYEtziISSDl"
      },
      "source": [
        "from transformers import AutoTokenizer,AutoModel, PreTrainedModel,PretrainedConfig\n",
        "from typing import Dict\n",
        "import torch\n",
        "\n",
        "class ColBERTConfig(PretrainedConfig):\n",
        "    model_type = \"ColBERT\"\n",
        "    bert_model: str\n",
        "    compression_dim: int = 768\n",
        "    dropout: float = 0.0\n",
        "    return_vecs: bool = False\n",
        "    trainable: bool = True\n",
        "\n",
        "class ColBERT(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    ColBERT model from: https://arxiv.org/pdf/2004.12832.pdf\n",
        "    We use a dot-product instead of cosine per term (slightly better)\n",
        "    \"\"\"\n",
        "    config_class = ColBERTConfig\n",
        "    base_model_prefix = \"bert_model\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 cfg) -> None:\n",
        "        super().__init__(cfg)\n",
        "        \n",
        "        self.bert_model = AutoModel.from_pretrained(cfg.bert_model)\n",
        "\n",
        "        for p in self.bert_model.parameters():\n",
        "            p.requires_grad = cfg.trainable\n",
        "\n",
        "        self.compressor = torch.nn.Linear(self.bert_model.config.hidden_size, cfg.compression_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                query: Dict[str, torch.LongTensor],\n",
        "                document: Dict[str, torch.LongTensor]):\n",
        "\n",
        "        query_vecs = self.forward_representation(query)\n",
        "        document_vecs = self.forward_representation(document)\n",
        "\n",
        "        score = self.forward_aggregation(query_vecs,document_vecs,query[\"attention_mask\"],document[\"attention_mask\"])\n",
        "        return score\n",
        "\n",
        "    def forward_representation(self,\n",
        "                               tokens,\n",
        "                               sequence_type=None) -> torch.Tensor:\n",
        "        \n",
        "        vecs = self.bert_model(**tokens)[0] # assuming a distilbert model here\n",
        "        vecs = self.compressor(vecs)\n",
        "\n",
        "        # if encoding only, zero-out the mask values so we can compress storage\n",
        "        if sequence_type == \"doc_encode\" or sequence_type == \"query_encode\": \n",
        "            vecs = vecs * tokens[\"tokens\"][\"mask\"].unsqueeze(-1)\n",
        "\n",
        "        return vecs\n",
        "\n",
        "    def forward_aggregation(self,query_vecs, document_vecs,query_mask,document_mask):\n",
        "        \n",
        "        # create initial term-x-term scores (dot-product)\n",
        "        score = torch.bmm(query_vecs, document_vecs.transpose(2,1))\n",
        "\n",
        "        # mask out padding on the doc dimension (mask by -1000, because max should not select those, setting it to 0 might select them)\n",
        "        exp_mask = document_mask.bool().unsqueeze(1).expand(-1,score.shape[1],-1)\n",
        "        score[~exp_mask] = - 10000\n",
        "\n",
        "        # max pooling over document dimension\n",
        "        score = score.max(-1).values\n",
        "\n",
        "        # mask out paddding query values\n",
        "        score[~(query_mask.bool())] = 0\n",
        "\n",
        "        # sum over query values\n",
        "        score = score.sum(-1)\n",
        "\n",
        "        return score\n",
        "\n",
        "#\n",
        "# init the model & tokenizer (using the distilbert tokenizer)\n",
        "#\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # honestly not sure if that is the best way to go, but it works :)\n",
        "model = ColBERT.from_pretrained(\"sebastian-hofstaetter/colbert-distilbert-margin_mse-T2-msmarco\")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOGT8YQQX1Ot"
      },
      "source": [
        "Now we are ready to use the model to encode two sample passage and query pairs (this would be the re-ranking mode, where we have a candidate list - for indexing or pre-compute mode you need to call forward_representation and forward_aggregation independently):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzt9Ix9UYMLy",
        "outputId": "529e338e-b4e7-4251-cf9b-4363ac8a3ed8"
      },
      "source": [
        "# our relevant example\n",
        "passage1_input = tokenizer(\"We are very happy to show you the ü§ó Transformers library for pre-trained language models. We are helping the community work together towards the goal of advancing NLP üî•.\",return_tensors=\"pt\")\n",
        "# a non-relevant example\n",
        "passage2_input = tokenizer(\"Hmm I don't like this new movie about transformers that i got from my local library. Those transformers are robots?\",return_tensors=\"pt\")\n",
        "\n",
        "# the user query -> which should give us a better score for the first passage\n",
        "query_input = tokenizer(\"what is the transformers library\")\n",
        "# adding the mask augmentation, we used 8 as the fixed number for training regardless of batch-size\n",
        "# it has a somewhat (although not huge) positive impact on effectiveness, we hypothesize that might be due to the increased\n",
        "# capacity of the query encoding, not so much because of the [MASK] pre-training, but who knows :)\n",
        "query_input.input_ids += [101] * 8 # [MASK]\n",
        "query_input.attention_mask += [1] * 8\n",
        "query_input[\"input_ids\"] = torch.LongTensor(query_input.input_ids).unsqueeze(0)\n",
        "query_input[\"attention_mask\"] = torch.LongTensor(query_input.attention_mask).unsqueeze(0)\n",
        "\n",
        "#print(\"Passage 1 Tokenized:\",passage1_input)\n",
        "#print(\"Passage 2 Tokenized:\",passage2_input)\n",
        "#print(\"Query Tokenized:\",query_input)\n",
        "\n",
        "# note how we call the bert model for pairs, can be changed to: forward_representation and forward_aggregation\n",
        "score_for_p1 = model.forward(query_input,passage1_input).squeeze(0)\n",
        "score_for_p2 = model.forward(query_input,passage2_input).squeeze(0)\n",
        "\n",
        "print(\"---\")\n",
        "print(\"Score passage 1 <-> query: \",float(score_for_p1))\n",
        "print(\"Score passage 2 <-> query: \",float(score_for_p2))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Score passage 1 <-> query:  104.63861083984375\n",
            "Score passage 2 <-> query:  101.60920715332031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1bY5qB9b-AI"
      },
      "source": [
        "As we see the model gives the first passage a higher score than the second - these scores would now be used to generate a list (if we run this comparison on all passages in our collection or candidate set). The scores are in the 100+ range (as we create a dot-product of 768 dimensional vectors, which naturally gives a larger score)\n",
        "\n",
        "- If you want to look at more complex usages and training code we have a library for that: https://github.com/sebastian-hofstaetter/transformer-kernel-ranking üëè\n",
        "\n",
        "- If you use our model checkpoint please cite our work as:\n",
        "\n",
        "    ```\n",
        "@misc{hofstaetter2020_crossarchitecture_kd,\n",
        "      title={Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation}, \n",
        "      author={Sebastian Hofst{\\\"a}tter and Sophia Althammer and Michael Schr{\\\"o}der and Mete Sertkan and Allan Hanbury},\n",
        "      year={2020},\n",
        "      eprint={2010.02666},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.IR}\n",
        "}\n",
        "    ```\n",
        "\n",
        "Thank You üòä If you have any questions feel free to reach out to Sebastian via mail (email in the paper). \n"
      ]
    }
  ]
}